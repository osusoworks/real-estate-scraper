import os
import sys
import requests
import json
import pandas as pd
from bs4 import BeautifulSoup
import time
import urllib.parse
import threading
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
from datetime import datetime

LIST_URL = 'https://shiraoka-housedo.com/list/'
BASE_URL = 'https://shiraoka-housedo.com'
IMG_FOLDER = 'images'
CSV_FILE = 'export.csv'
JSON_FILE = 'export.json'
HISTORY_FILE = 'scraping_history.json'  # ÂèñÂæóÂ±•Ê≠¥„Éï„Ç°„Ç§„É´

# „É¶„Éº„Ç∂„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà‰ªò„ÅçÂÖ±ÈÄö„Éò„ÉÉ„ÉÄ
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36",
    "Accept-Language": "ja,en-US;q=0.9",
    "Referer": "https://shiraoka-housedo.com/list/"
}

class ScraperGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("‰∏çÂãïÁî£„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„ÉÑ„Éº„É´ v2")
        self.root.geometry("800x650")
        self.root.resizable(True, True)
        
        self.is_running = False
        self.scraped_ids = self.load_history()  # ÂèñÂæóÊ∏à„ÅøÁâ©‰ª∂Áï™Âè∑
        self.create_widgets()
        
    def load_history(self):
        """ÂèñÂæóÂ±•Ê≠¥„ÇíË™≠„ÅøËæº„ÇÄ"""
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('scraped_ids', []))
            except Exception:
                return set()
        return set()
    
    def save_history(self):
        """ÂèñÂæóÂ±•Ê≠¥„Çí‰øùÂ≠ò"""
        try:
            data = {
                'scraped_ids': list(self.scraped_ids),
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as ex:
            print(f'Â±•Ê≠¥‰øùÂ≠ò„Ç®„É©„Éº: {ex}')
        
    def create_widgets(self):
        # „Çø„Ç§„Éà„É´
        title_frame = tk.Frame(self.root, bg="#2c3e50", height=60)
        title_frame.pack(fill=tk.X)
        title_frame.pack_propagate(False)
        
        title_label = tk.Label(
            title_frame, 
            text="üè† ‰∏çÂãïÁî£„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„ÉÑ„Éº„É´ v2", 
            font=("Arial", 16, "bold"),
            bg="#2c3e50",
            fg="white"
        )
        title_label.pack(pady=15)
        
        # Ë®≠ÂÆö„Éï„É¨„Éº„É†
        settings_frame = tk.LabelFrame(self.root, text="Ë®≠ÂÆö", font=("Arial", 10, "bold"), padx=20, pady=15)
        settings_frame.pack(fill=tk.X, padx=20, pady=10)
        
        # ÂèñÂæó‰ª∂Êï∞
        items_frame = tk.Frame(settings_frame)
        items_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(items_frame, text="ÂèñÂæó‰ª∂Êï∞:", font=("Arial", 10)).pack(side=tk.LEFT)
        
        self.max_items_var = tk.StringVar(value="15")
        self.max_items_entry = tk.Entry(items_frame, textvariable=self.max_items_var, width=10, font=("Arial", 10))
        self.max_items_entry.pack(side=tk.LEFT, padx=10)
        
        tk.Label(items_frame, text="‰ª∂ÔºàÂÖ®‰ª∂ÂèñÂæó„Åô„ÇãÂ†¥Âêà„ÅØÂ§ß„Åç„Å™Êï∞ÂÄ§„ÇíÂÖ•ÂäõÔºâ", font=("Arial", 9), fg="gray").pack(side=tk.LEFT)
        
        # „Çπ„Ç≠„ÉÉ„ÉóË®≠ÂÆö
        skip_frame = tk.Frame(settings_frame)
        skip_frame.pack(fill=tk.X, pady=5)
        
        self.skip_scraped_var = tk.BooleanVar(value=True)
        self.skip_checkbox = tk.Checkbutton(
            skip_frame,
            text="ÂèñÂæóÊ∏à„ÅøÁâ©‰ª∂„Çí„Çπ„Ç≠„ÉÉ„Éó„Åô„Çã",
            variable=self.skip_scraped_var,
            font=("Arial", 10)
        )
        self.skip_checkbox.pack(side=tk.LEFT)
        
        # ÂèñÂæóÊ∏à„Åø‰ª∂Êï∞Ë°®Á§∫
        self.scraped_count_label = tk.Label(
            skip_frame,
            text=f"ÔºàÂèñÂæóÊ∏à„Åø: {len(self.scraped_ids)}‰ª∂Ôºâ",
            font=("Arial", 9),
            fg="gray"
        )
        self.scraped_count_label.pack(side=tk.LEFT, padx=10)
        
        # Â±•Ê≠¥„ÇØ„É™„Ç¢„Éú„Çø„É≥
        self.clear_history_button = tk.Button(
            skip_frame,
            text="Â±•Ê≠¥„ÇØ„É™„Ç¢",
            command=self.clear_history,
            font=("Arial", 9),
            bg="#e67e22",
            fg="white",
            cursor="hand2"
        )
        self.clear_history_button.pack(side=tk.LEFT, padx=5)
        
        # URLË°®Á§∫
        url_frame = tk.Frame(settings_frame)
        url_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(url_frame, text="ÂØæË±°„Çµ„Ç§„Éà:", font=("Arial", 10)).pack(side=tk.LEFT)
        tk.Label(url_frame, text=LIST_URL, font=("Arial", 9), fg="blue").pack(side=tk.LEFT, padx=10)
        
        # „Éú„Çø„É≥„Éï„É¨„Éº„É†
        button_frame = tk.Frame(self.root)
        button_frame.pack(pady=10)
        
        self.start_button = tk.Button(
            button_frame,
            text="„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã",
            command=self.start_scraping,
            bg="#27ae60",
            fg="white",
            font=("Arial", 11, "bold"),
            width=20,
            height=2,
            cursor="hand2"
        )
        self.start_button.pack(side=tk.LEFT, padx=5)
        
        self.stop_button = tk.Button(
            button_frame,
            text="ÂÅúÊ≠¢",
            command=self.stop_scraping,
            bg="#e74c3c",
            fg="white",
            font=("Arial", 11, "bold"),
            width=15,
            height=2,
            state=tk.DISABLED,
            cursor="hand2"
        )
        self.stop_button.pack(side=tk.LEFT, padx=5)
        
        # ÈÄ≤Êçó„Éê„Éº
        progress_frame = tk.Frame(self.root)
        progress_frame.pack(fill=tk.X, padx=20, pady=5)
        
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(
            progress_frame,
            variable=self.progress_var,
            maximum=100,
            mode='determinate',
            length=400
        )
        self.progress_bar.pack(fill=tk.X)
        
        self.progress_label = tk.Label(progress_frame, text="ÂæÖÊ©ü‰∏≠...", font=("Arial", 9))
        self.progress_label.pack(pady=5)
        
        # „É≠„Ç∞Ë°®Á§∫
        log_frame = tk.LabelFrame(self.root, text="ÂÆüË°å„É≠„Ç∞", font=("Arial", 10, "bold"), padx=10, pady=10)
        log_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)
        
        self.log_text = scrolledtext.ScrolledText(
            log_frame,
            wrap=tk.WORD,
            width=80,
            height=15,
            font=("Consolas", 9),
            bg="#f8f9fa",
            fg="#2c3e50"
        )
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # „Çπ„ÉÜ„Éº„Çø„Çπ„Éê„Éº
        status_frame = tk.Frame(self.root, bg="#34495e", height=30)
        status_frame.pack(fill=tk.X, side=tk.BOTTOM)
        status_frame.pack_propagate(False)
        
        self.status_label = tk.Label(
            status_frame,
            text="Ê∫ñÂÇôÂÆå‰∫Ü",
            font=("Arial", 9),
            bg="#34495e",
            fg="white",
            anchor=tk.W
        )
        self.status_label.pack(fill=tk.X, padx=10, pady=5)
    
    def clear_history(self):
        """ÂèñÂæóÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢"""
        if messagebox.askyesno("Á¢∫Ë™ç", "ÂèñÂæóÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åô„ÅãÔºü\nÊ¨°ÂõûÂÆüË°åÊôÇ„ÄÅ„Åô„Åπ„Å¶„ÅÆÁâ©‰ª∂„ÇíÂÜçÂèñÂæó„Åó„Åæ„Åô„ÄÇ"):
            self.scraped_ids.clear()
            self.save_history()
            self.scraped_count_label.config(text=f"ÔºàÂèñÂæóÊ∏à„Åø: 0‰ª∂Ôºâ")
            self.log("‚úì ÂèñÂæóÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åó„Åü")
            messagebox.showinfo("ÂÆå‰∫Ü", "ÂèñÂæóÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åó„Åü")
        
    def log(self, message):
        """„É≠„Ç∞„ÇíË°®Á§∫"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
        self.log_text.see(tk.END)
        self.root.update_idletasks()
        
    def update_progress(self, current, total):
        """ÈÄ≤Êçó„Éê„Éº„ÇíÊõ¥Êñ∞"""
        if total > 0:
            progress = (current / total) * 100
            self.progress_var.set(progress)
            self.progress_label.config(text=f"{current}/{total} ‰ª∂Âá¶ÁêÜ‰∏≠... ({progress:.1f}%)")
        self.root.update_idletasks()
        
    def start_scraping(self):
        """„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã"""
        try:
            max_items = int(self.max_items_var.get())
            if max_items <= 0:
                messagebox.showerror("„Ç®„É©„Éº", "ÂèñÂæó‰ª∂Êï∞„ÅØ1‰ª•‰∏ä„ÅÆÊï∞ÂÄ§„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
                return
        except ValueError:
            messagebox.showerror("„Ç®„É©„Éº", "ÂèñÂæó‰ª∂Êï∞„ÅØÊï∞ÂÄ§„ÅßÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
            return
        
        self.is_running = True
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.max_items_entry.config(state=tk.DISABLED)
        self.skip_checkbox.config(state=tk.DISABLED)
        self.clear_history_button.config(state=tk.DISABLED)
        self.log_text.delete(1.0, tk.END)
        self.progress_var.set(0)
        self.status_label.config(text="ÂÆüË°å‰∏≠...")
        
        # Âà•„Çπ„É¨„ÉÉ„Éâ„ÅßÂÆüË°å
        thread = threading.Thread(target=self.run_scraping, args=(max_items,))
        thread.daemon = True
        thread.start()
        
    def stop_scraping(self):
        """„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÅúÊ≠¢"""
        self.is_running = False
        self.log("‚ö†Ô∏è ÂÅúÊ≠¢„É™„ÇØ„Ç®„Çπ„Éà„ÇíÂèó‰ø°„Åó„Åæ„Åó„Åü...")
        self.status_label.config(text="ÂÅúÊ≠¢‰∏≠...")
        
    def run_scraping(self, max_items):
        """„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆüË°å"""
        try:
            skip_scraped = self.skip_scraped_var.get()
            
            self.log("=" * 60)
            self.log("üöÄ ‰∏çÂãïÁî£„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã")
            if skip_scraped:
                self.log(f"üìã ÂèñÂæóÊ∏à„Åø„Çπ„Ç≠„ÉÉ„Éó„É¢„Éº„Éâ: ONÔºà{len(self.scraped_ids)}‰ª∂„Çπ„Ç≠„ÉÉ„ÉóÔºâ")
            else:
                self.log("üìã ÂèñÂæóÊ∏à„Åø„Çπ„Ç≠„ÉÉ„Éó„É¢„Éº„Éâ: OFFÔºà„Åô„Åπ„Å¶ÂèñÂæóÔºâ")
            self.log("=" * 60)
            
            # ‰∏ÄË¶ß„Éö„Éº„Ç∏„Åã„ÇâÁâ©‰ª∂URL„ÇíÂèñÂæó
            self.log("üìÑ ‰∏ÄË¶ß„Éö„Éº„Ç∏„ÇíÂèñÂæó‰∏≠...")
            list_soup = self.fetch_list_page()
            detail_urls = self.get_detail_urls(list_soup, max_items)
            
            if not detail_urls:
                self.log("‚ùå „Ç®„É©„Éº: Áâ©‰ª∂URL„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü")
                self.finish_scraping(False)
                return
            
            self.log(f"‚úì {len(detail_urls)}‰ª∂„ÅÆÁâ©‰ª∂URL„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\n")
            
            rows = []
            skipped_count = 0
            new_count = 0
            
            for idx, url in enumerate(detail_urls, 1):
                if not self.is_running:
                    self.log("‚ö†Ô∏è „É¶„Éº„Ç∂„Éº„Å´„Çà„Å£„Å¶ÂÅúÊ≠¢„Åï„Çå„Åæ„Åó„Åü")
                    break
                
                # Áâ©‰ª∂Áï™Âè∑„ÇíÊäΩÂá∫ÔºàURL„Åã„ÇâÔºâ
                estate_id = url.rstrip('/').split('/')[-1]
                
                # „Çπ„Ç≠„ÉÉ„Éó„ÉÅ„Çß„ÉÉ„ÇØ
                if skip_scraped and estate_id in self.scraped_ids:
                    self.log(f"[{idx}/{len(detail_urls)}] ‚è≠Ô∏è „Çπ„Ç≠„ÉÉ„Éó: Áâ©‰ª∂Áï™Âè∑ {estate_id}ÔºàÂèñÂæóÊ∏à„ÅøÔºâ")
                    skipped_count += 1
                    self.update_progress(idx, len(detail_urls))
                    continue
                
                self.log(f"[{idx}/{len(detail_urls)}] Âá¶ÁêÜ‰∏≠: {url}")
                self.update_progress(idx - 1, len(detail_urls))
                
                try:
                    # Ë©≥Á¥∞„Éö„Éº„Ç∏„ÇíÂèñÂæó
                    soup = self.fetch_detail_page(url)
                    
                    # Áâ©‰ª∂ÊÉÖÂ†±„ÇíÊäΩÂá∫
                    d = self.extract_detail(soup, url)
                    
                    # ÁîªÂÉè„ÇíÂèñÂæó
                    if d['Áâ©‰ª∂Áï™Âè∑']:
                        img_urls = self.extract_images(soup, d['Áâ©‰ª∂Áï™Âè∑'])
                        d['ÁîªÂÉèURL'] = ', '.join(img_urls) if img_urls else ''
                        d['ÁîªÂÉèÊûöÊï∞'] = len(img_urls)
                        
                        # ÂèñÂæóÊ∏à„Åø„É™„Çπ„Éà„Å´ËøΩÂä†
                        self.scraped_ids.add(d['Áâ©‰ª∂Áï™Âè∑'])
                        new_count += 1
                    else:
                        d['ÁîªÂÉèURL'] = ''
                        d['ÁîªÂÉèÊûöÊï∞'] = 0
                    
                    rows.append(d)
                    self.log(f"  ‚úì Áâ©‰ª∂Áï™Âè∑ {d['Áâ©‰ª∂Áï™Âè∑']} - ÁîªÂÉè{d['ÁîªÂÉèÊûöÊï∞']}ÊûöÂèñÂæó")
                    
                    # „Ç¢„ÇØ„Çª„ÇπÈñìÈöî„ÇíÁ©∫„Åë„Çã
                    if idx < len(detail_urls):
                        time.sleep(1)
                    
                except Exception as ex:
                    self.log(f"  ‚ùå „Ç®„É©„Éº: {ex}")
                    continue
            
            self.update_progress(len(detail_urls), len(detail_urls))
            
            # Â±•Ê≠¥„Çí‰øùÂ≠ò
            self.save_history()
            self.scraped_count_label.config(text=f"ÔºàÂèñÂæóÊ∏à„Åø: {len(self.scraped_ids)}‰ª∂Ôºâ")
            
            if not rows:
                self.log("\n‚ö†Ô∏è Êñ∞Ë¶è„Éá„Éº„Çø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü")
                self.log(f"  „Çπ„Ç≠„ÉÉ„Éó: {skipped_count}‰ª∂")
                self.finish_scraping(True)
                return
            
            # CSV/JSONÂá∫Âäõ
            self.log("\nüíæ „Éá„Éº„Çø„ÇíÂá∫Âäõ‰∏≠...")
            
            # Êó¢Â≠ò„Éá„Éº„Çø„Å®ÁµêÂêà
            if os.path.exists(CSV_FILE):
                existing_df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')
                new_df = pd.DataFrame(rows)
                df = pd.concat([existing_df, new_df], ignore_index=True)
                # Áâ©‰ª∂Áï™Âè∑„ÅßÈáçË§áÂâäÈô§ÔºàÊúÄÊñ∞„Çí‰øùÊåÅÔºâ
                df = df.drop_duplicates(subset=['Áâ©‰ª∂Áï™Âè∑'], keep='last')
            else:
                df = pd.DataFrame(rows)
            
            df.to_csv(CSV_FILE, index=False, encoding='utf-8-sig')
            self.log(f"  ‚úì CSVÂá∫ÂäõÂÆå‰∫Ü: {CSV_FILE}")
            
            # JSONÂá∫Âäõ
            all_data = df.to_dict('records')
            with open(JSON_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            self.log(f"  ‚úì JSONÂá∫ÂäõÂÆå‰∫Ü: {JSON_FILE}")
            
            # ÁîªÂÉè„Éï„Ç©„É´„ÉÄ„ÅÆÁ¢∫Ë™ç
            if os.path.exists(IMG_FOLDER):
                img_files = os.listdir(IMG_FOLDER)
                self.log(f"  ‚úì ÁîªÂÉè‰øùÂ≠òÂÆå‰∫Ü: {IMG_FOLDER}/ ({len(img_files)}„Éï„Ç°„Ç§„É´)")
            
            self.log("\n" + "=" * 60)
            self.log(f"üéâ ÂÆå‰∫Ü:")
            self.log(f"  Êñ∞Ë¶èÂèñÂæó: {new_count}‰ª∂")
            self.log(f"  „Çπ„Ç≠„ÉÉ„Éó: {skipped_count}‰ª∂")
            self.log(f"  ÂêàË®à: {len(df)}‰ª∂ÔºàCSV/JSONÔºâ")
            self.log("=" * 60)
            
            self.finish_scraping(True)
            
        except Exception as ex:
            self.log(f"\n‚ùå „Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {ex}")
            self.finish_scraping(False)
    
    def finish_scraping(self, success):
        """„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÁµÇ‰∫ÜÂá¶ÁêÜ"""
        self.is_running = False
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
        self.max_items_entry.config(state=tk.NORMAL)
        self.skip_checkbox.config(state=tk.NORMAL)
        self.clear_history_button.config(state=tk.NORMAL)
        
        if success:
            self.status_label.config(text="ÂÆå‰∫Ü")
            messagebox.showinfo("ÂÆå‰∫Ü", "„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ\n\nÂá∫Âäõ„Éï„Ç°„Ç§„É´:\n- export.csv\n- export.json\n- images/")
            
            # ÁµêÊûú„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè
            if messagebox.askyesno("Á¢∫Ë™ç", "ÁµêÊûú„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åç„Åæ„Åô„ÅãÔºü"):
                self.open_output_folder()
        else:
            self.status_label.config(text="„Ç®„É©„Éº")
    
    def open_output_folder(self):
        """Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè"""
        try:
            if sys.platform == 'win32':
                os.startfile(os.getcwd())
            elif sys.platform == 'darwin':
                os.system(f'open "{os.getcwd()}"')
            else:
                os.system(f'xdg-open "{os.getcwd()}"')
        except Exception as ex:
            self.log(f"„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åë„Åæ„Åõ„Çì„Åß„Åó„Åü: {ex}")
    
    # ‰ª•‰∏ã„ÄÅ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞Èñ¢ÈÄ£„ÅÆ„É°„ÇΩ„ÉÉ„Éâ
    
    def fetch_list_page(self):
        res = requests.get(LIST_URL, headers=HEADERS)
        return BeautifulSoup(res.text, 'html.parser')
    
    def get_detail_urls(self, soup, max_items):
        urls = []
        scripts = soup.find_all('script', {'type':'application/ld+json'})
        for script in scripts:
            if 'ItemList' in script.text:
                try:
                    data = json.loads(script.text)
                    for item in data.get('itemListElement', []):
                        url = item.get('item')
                        urls.append(url)
                        if len(urls) >= max_items:
                            break
                except Exception:
                    pass
        return urls
    
    def fetch_detail_page(self, url):
        detail_headers = HEADERS.copy()
        detail_headers["Referer"] = LIST_URL
        res = requests.get(url, headers=detail_headers)
        return BeautifulSoup(res.text, 'html.parser')
    
    def extract_images(self, soup, estate_id):
        img_urls = []
        
        # „ÇÆ„É£„É©„É™„Éº„Çø„Ç∞„Åã„ÇâÂèñÂæó
        img_tags = soup.select('a.mainContents_gallery-colorbox')
        
        # preload„Çø„Ç∞„Åã„ÇâÂèñÂæó
        preload_tags = soup.select('link[rel="preload"][as="image"]')
        
        if img_tags:
            for tag in img_tags:
                href = tag.get('href')
                if href:
                    img_urls.append(href)
        elif estate_id and preload_tags:
            for tag in preload_tags:
                href = tag.get('href', '')
                if estate_id in href and 'exterior' in href.lower():
                    img_urls.append(href)
        
        for idx, img_url in enumerate(img_urls):
            img_url = urllib.parse.urljoin(BASE_URL, img_url)
            
            try:
                res = requests.get(img_url, headers=HEADERS, timeout=10)
                
                if res.status_code == 200:
                    img_ext = img_url.split('.')[-1].split('?')[0]
                    if img_ext not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                        img_ext = 'jpg'
                    
                    img_name = f"{estate_id}_{idx+1}.{img_ext}"
                    img_dir = IMG_FOLDER
                    os.makedirs(img_dir, exist_ok=True)
                    
                    img_path = os.path.join(img_dir, img_name)
                    with open(img_path, 'wb') as f:
                        f.write(res.content)
            except Exception:
                pass
            
            time.sleep(0.5)
        
        return [urllib.parse.urljoin(BASE_URL, url) for url in img_urls]
    
    def extract_detail(self, soup, url):
        estate_elem = soup.find('td', class_='estateID')
        estate_id = estate_elem.text.strip() if estate_elem else ''
        
        table = soup.find('table')
        data = {}
        
        if table:
            all_cells = table.find_all(['th', 'td'])
            
            i = 0
            while i < len(all_cells):
                cell = all_cells[i]
                
                if cell.name == 'th':
                    key = cell.text.strip()
                    j = i + 1
                    while j < len(all_cells):
                        next_cell = all_cells[j]
                        if next_cell.name == 'td':
                            value = next_cell.text.strip()
                            data[key] = value
                            i = j
                            break
                        elif next_cell.name == 'th':
                            break
                        j += 1
                
                i += 1
        
        summary = {
            'Áâ©‰ª∂Áï™Âè∑': estate_id,
            '‰æ°Ê†º': data.get('‰æ°Ê†º', ''),
            'ÈñìÂèñ„Çä': data.get('ÈñìÂèñ„Çä', ''),
            'Áâ©‰ª∂Á®ÆÂà•': data.get('Áâ©‰ª∂Á®ÆÂà•', ''),
            'ÊâÄÂú®Âú∞': data.get('ÊâÄÂú®Âú∞', ''),
            '„Ç¢„ÇØ„Çª„Çπ': data.get('„Ç¢„ÇØ„Çª„Çπ', ''),
            'Âª∫Áâ©Èù¢Á©ç': data.get('Âª∫Áâ©Èù¢Á©ç', ''),
            'ÈßêËªäÂ†¥': data.get('ÈßêËªäÂ†¥', ''),
            'ÁØâÂπ¥Êúà': data.get('ÁØâÂπ¥Êúà', ''),
            'Âª∫Áâ©ÊßãÈÄ†': data.get('Âª∫Áâ©ÊßãÈÄ†', ''),
            'Â∑•Ê≥ï': data.get('Â∑•Ê≥ï', ''),
            '‰∏ªË¶ÅÊé°ÂÖâ': data.get('‰∏ªË¶ÅÊé°ÂÖâ', ''),
            '„Éê„É´„Ç≥„Éã„Éº': data.get('„Éê„É´„Ç≥„Éã„Éº', ''),
            '‰øùË®º„ÉªË©ï‰æ°': data.get('‰øùË®º„ÉªË©ï‰æ°', ''),
            '„É™„Éï„Ç©„Éº„É†': data.get('„É™„Éï„Ç©„Éº„É†', ''),
            'ÂúüÂú∞Èù¢Á©ç': data.get('ÂúüÂú∞Èù¢Á©ç', ''),
            'Êé•ÈÅì': data.get('Êé•ÈÅì', ''),
            '„Çª„ÉÉ„Éà„Éê„ÉÉ„ÇØ': data.get('„Çª„ÉÉ„Éà„Éê„ÉÉ„ÇØ', ''),
            'ÁßÅÈÅì': data.get('ÁßÅÈÅì', ''),
            'Âú∞ÁõÆ': data.get('Âú∞ÁõÆ', ''),
            'Âú∞Âã¢': data.get('Âú∞Âã¢', ''),
            'Ê®©Âà©': data.get('Ê®©Âà©', ''),
            'ÈÉΩÂ∏ÇË®àÁîª': data.get('ÈÉΩÂ∏ÇË®àÁîª', ''),
            'Áî®ÈÄîÂú∞Âüü': data.get('Áî®ÈÄîÂú∞Âüü', ''),
            'Âª∫„Å∫„ÅÑ/ÂÆπÁ©çÁéá': data.get('Âª∫„Å∫„ÅÑ/ÂÆπÁ©çÁéá', ''),
            'ÂúüÂú∞ÂõΩÂúüÊ≥ï': data.get('ÂúüÂú∞ÂõΩÂúüÊ≥ï', ''),
            'Ë®±ÂèØÁï™Âè∑': data.get('Ë®±ÂèØÁï™Âè∑', ''),
            'Âª∫ÁØâÂü∫Ê∫ñÊ≥ï': data.get('Âª∫ÁØâÂü∫Ê∫ñÊ≥ï', ''),
            'Ê≥ï‰ª§Âà∂Èôê': data.get('Ê≥ï‰ª§Âà∂Èôê', ''),
            'Â∞èÂ≠¶Âå∫': data.get('Â∞èÂ≠¶Âå∫', ''),
            '‰∏≠Â≠¶Âå∫': data.get('‰∏≠Â≠¶Âå∫', ''),
            'ÁèæÊ≥Å': data.get('ÁèæÊ≥Å', ''),
            'ÂºïÊ∏°': data.get('ÂºïÊ∏°', ''),
            '„Åù„ÅÆ‰ªñË≤ªÁî®': data.get('„Åù„ÅÆ‰ªñË≤ªÁî®', ''),
            'ÂÇôËÄÉ': data.get('ÂÇôËÄÉ', ''),
            'ÂèñÂºïÊÖãÊßò': data.get('ÂèñÂºïÊÖãÊßò', ''),
            'Ë©≥Á¥∞„Éö„Éº„Ç∏': url
        }
        return summary


def main():
    root = tk.Tk()
    app = ScraperGUI(root)
    root.mainloop()


if __name__ == '__main__':
    main()


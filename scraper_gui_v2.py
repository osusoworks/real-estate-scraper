import os
import sys
import requests
import json
import pandas as pd
from bs4 import BeautifulSoup
import time
import urllib.parse
import threading
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
from datetime import datetime

LIST_URL = 'https://shiraoka-housedo.com/list/'
BASE_URL = 'https://shiraoka-housedo.com'
IMG_FOLDER = 'images'
CSV_FILE = 'export.csv'
JSON_FILE = 'export.json'
HISTORY_FILE = 'scraping_history.json'  # å–å¾—å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä»˜ãå…±é€šãƒ˜ãƒƒãƒ€
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36",
    "Accept-Language": "ja,en-US;q=0.9",
    "Referer": "https://shiraoka-housedo.com/list/"
}

class ScraperGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("ä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v2")
        self.root.geometry("800x650")
        self.root.resizable(True, True)
        
        self.is_running = False
        self.scraped_ids = self.load_history()  # å–å¾—æ¸ˆã¿ç‰©ä»¶ç•ªå·
        self.create_widgets()
        
    def load_history(self):
        """å–å¾—å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('scraped_ids', []))
            except Exception:
                return set()
        return set()
    
    def save_history(self):
        """å–å¾—å±¥æ­´ã‚’ä¿å­˜"""
        try:
            data = {
                'scraped_ids': list(self.scraped_ids),
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as ex:
            print(f'å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {ex}')
        
    def create_widgets(self):
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_frame = tk.Frame(self.root, bg="#2c3e50", height=60)
        title_frame.pack(fill=tk.X)
        title_frame.pack_propagate(False)
        
        title_label = tk.Label(
            title_frame, 
            text="ğŸ  ä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v2", 
            font=("Arial", 16, "bold"),
            bg="#2c3e50",
            fg="white"
        )
        title_label.pack(pady=15)
        
        # è¨­å®šãƒ•ãƒ¬ãƒ¼ãƒ 
        settings_frame = tk.LabelFrame(self.root, text="è¨­å®š", font=("Arial", 10, "bold"), padx=20, pady=15)
        settings_frame.pack(fill=tk.X, padx=20, pady=10)
        
        # å–å¾—ä»¶æ•°
        items_frame = tk.Frame(settings_frame)
        items_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(items_frame, text="å–å¾—ä»¶æ•°:", font=("Arial", 10)).pack(side=tk.LEFT)
        
        self.max_items_var = tk.StringVar(value="15")
        self.max_items_entry = tk.Entry(items_frame, textvariable=self.max_items_var, width=10, font=("Arial", 10))
        self.max_items_entry.pack(side=tk.LEFT, padx=10)
        
        tk.Label(items_frame, text="ä»¶ï¼ˆå…¨ä»¶å–å¾—ã™ã‚‹å ´åˆã¯å¤§ããªæ•°å€¤ã‚’å…¥åŠ›ï¼‰", font=("Arial", 9), fg="gray").pack(side=tk.LEFT)
        
        # ã‚¹ã‚­ãƒƒãƒ—è¨­å®š
        skip_frame = tk.Frame(settings_frame)
        skip_frame.pack(fill=tk.X, pady=5)
        
        self.skip_scraped_var = tk.BooleanVar(value=True)
        self.skip_checkbox = tk.Checkbutton(
            skip_frame,
            text="å–å¾—æ¸ˆã¿ç‰©ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹",
            variable=self.skip_scraped_var,
            font=("Arial", 10)
        )
        self.skip_checkbox.pack(side=tk.LEFT)
        
        # å–å¾—æ¸ˆã¿ä»¶æ•°è¡¨ç¤º
        self.scraped_count_label = tk.Label(
            skip_frame,
            text=f"ï¼ˆå–å¾—æ¸ˆã¿: {len(self.scraped_ids)}ä»¶ï¼‰",
            font=("Arial", 9),
            fg="gray"
        )
        self.scraped_count_label.pack(side=tk.LEFT, padx=10)
        
        # å±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
        self.clear_history_button = tk.Button(
            skip_frame,
            text="å±¥æ­´ã‚¯ãƒªã‚¢",
            command=self.clear_history,
            font=("Arial", 9),
            bg="#e67e22",
            fg="white",
            cursor="hand2"
        )
        self.clear_history_button.pack(side=tk.LEFT, padx=5)
        
        # URLè¡¨ç¤º
        url_frame = tk.Frame(settings_frame)
        url_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(url_frame, text="å¯¾è±¡ã‚µã‚¤ãƒˆ:", font=("Arial", 10)).pack(side=tk.LEFT)
        tk.Label(url_frame, text=LIST_URL, font=("Arial", 9), fg="blue").pack(side=tk.LEFT, padx=10)
        
        # ãƒœã‚¿ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        button_frame = tk.Frame(self.root)
        button_frame.pack(pady=10)
        
        self.start_button = tk.Button(
            button_frame,
            text="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹",
            command=self.start_scraping,
            bg="#27ae60",
            fg="white",
            font=("Arial", 11, "bold"),
            width=20,
            height=2,
            cursor="hand2"
        )
        self.start_button.pack(side=tk.LEFT, padx=5)
        
        self.stop_button = tk.Button(
            button_frame,
            text="åœæ­¢",
            command=self.stop_scraping,
            bg="#e74c3c",
            fg="white",
            font=("Arial", 11, "bold"),
            width=15,
            height=2,
            state=tk.DISABLED,
            cursor="hand2"
        )
        self.stop_button.pack(side=tk.LEFT, padx=5)
        
        # é€²æ—ãƒãƒ¼
        progress_frame = tk.Frame(self.root)
        progress_frame.pack(fill=tk.X, padx=20, pady=5)
        
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(
            progress_frame,
            variable=self.progress_var,
            maximum=100,
            mode='determinate',
            length=400
        )
        self.progress_bar.pack(fill=tk.X)
        
        self.progress_label = tk.Label(progress_frame, text="å¾…æ©Ÿä¸­...", font=("Arial", 9))
        self.progress_label.pack(pady=5)
        
        # ãƒ­ã‚°è¡¨ç¤º
        log_frame = tk.LabelFrame(self.root, text="å®Ÿè¡Œãƒ­ã‚°", font=("Arial", 10, "bold"), padx=10, pady=10)
        log_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)
        
        self.log_text = scrolledtext.ScrolledText(
            log_frame,
            wrap=tk.WORD,
            width=80,
            height=15,
            font=("Consolas", 9),
            bg="#f8f9fa",
            fg="#2c3e50"
        )
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒ¼
        status_frame = tk.Frame(self.root, bg="#34495e", height=30)
        status_frame.pack(fill=tk.X, side=tk.BOTTOM)
        status_frame.pack_propagate(False)
        
        self.status_label = tk.Label(
            status_frame,
            text="æº–å‚™å®Œäº†",
            font=("Arial", 9),
            bg="#34495e",
            fg="white",
            anchor=tk.W
        )
        self.status_label.pack(fill=tk.X, padx=10, pady=5)
    
    def clear_history(self):
        """å–å¾—å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        if messagebox.askyesno("ç¢ºèª", "å–å¾—å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã‹ï¼Ÿ\næ¬¡å›å®Ÿè¡Œæ™‚ã€ã™ã¹ã¦ã®ç‰©ä»¶ã‚’å†å–å¾—ã—ã¾ã™ã€‚"):
            self.scraped_ids.clear()
            self.save_history()
            self.scraped_count_label.config(text=f"ï¼ˆå–å¾—æ¸ˆã¿: 0ä»¶ï¼‰")
            self.log("âœ“ å–å¾—å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            messagebox.showinfo("å®Œäº†", "å–å¾—å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        
    def log(self, message):
        """ãƒ­ã‚°ã‚’è¡¨ç¤º"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
        self.log_text.see(tk.END)
        self.root.update_idletasks()
        
    def update_progress(self, current, total):
        """é€²æ—ãƒãƒ¼ã‚’æ›´æ–°"""
        if total > 0:
            progress = (current / total) * 100
            self.progress_var.set(progress)
            self.progress_label.config(text=f"{current}/{total} ä»¶å‡¦ç†ä¸­... ({progress:.1f}%)")
        self.root.update_idletasks()
        
    def start_scraping(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹"""
        try:
            max_items = int(self.max_items_var.get())
            if max_items <= 0:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "å–å¾—ä»¶æ•°ã¯1ä»¥ä¸Šã®æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                return
        except ValueError:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "å–å¾—ä»¶æ•°ã¯æ•°å€¤ã§å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        self.is_running = True
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.max_items_entry.config(state=tk.DISABLED)
        self.skip_checkbox.config(state=tk.DISABLED)
        self.clear_history_button.config(state=tk.DISABLED)
        self.log_text.delete(1.0, tk.END)
        self.progress_var.set(0)
        self.status_label.config(text="å®Ÿè¡Œä¸­...")
        
        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        thread = threading.Thread(target=self.run_scraping, args=(max_items,))
        thread.daemon = True
        thread.start()
        
    def stop_scraping(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åœæ­¢"""
        self.is_running = False
        self.log("âš ï¸ åœæ­¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡ã—ã¾ã—ãŸ...")
        self.status_label.config(text="åœæ­¢ä¸­...")
        
    def run_scraping(self, max_items):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        try:
            skip_scraped = self.skip_scraped_var.get()
            
            self.log("=" * 60)
            self.log("ğŸš€ ä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            if skip_scraped:
                self.log(f"ğŸ“‹ å–å¾—æ¸ˆã¿ã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰: ONï¼ˆ{len(self.scraped_ids)}ä»¶ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            else:
                self.log("ğŸ“‹ å–å¾—æ¸ˆã¿ã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰: OFFï¼ˆã™ã¹ã¦å–å¾—ï¼‰")
            self.log("=" * 60)
            
            # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰©ä»¶URLã‚’å–å¾—
            self.log("ğŸ“„ ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
            list_soup = self.fetch_list_page()
            detail_urls = self.get_detail_urls(list_soup, max_items)
            
            if not detail_urls:
                self.log("âŒ ã‚¨ãƒ©ãƒ¼: ç‰©ä»¶URLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                self.finish_scraping(False)
                return
            
            self.log(f"âœ“ {len(detail_urls)}ä»¶ã®ç‰©ä»¶URLã‚’å–å¾—ã—ã¾ã—ãŸ\n")
            
            rows = []
            skipped_count = 0
            new_count = 0
            
            for idx, url in enumerate(detail_urls, 1):
                if not self.is_running:
                    self.log("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦åœæ­¢ã•ã‚Œã¾ã—ãŸ")
                    break
                
                # ç‰©ä»¶ç•ªå·ã‚’æŠ½å‡ºï¼ˆURLã‹ã‚‰ï¼‰
                estate_id = url.rstrip('/').split('/')[-1]
                
                # ã‚¹ã‚­ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯
                if skip_scraped and estate_id in self.scraped_ids:
                    self.log(f"[{idx}/{len(detail_urls)}] â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: ç‰©ä»¶ç•ªå· {estate_id}ï¼ˆå–å¾—æ¸ˆã¿ï¼‰")
                    skipped_count += 1
                    self.update_progress(idx, len(detail_urls))
                    continue
                
                self.log(f"[{idx}/{len(detail_urls)}] å‡¦ç†ä¸­: {url}")
                self.update_progress(idx - 1, len(detail_urls))
                
                try:
                    # è©³ç´°ãƒšãƒ¼ã‚¸ã‚’å–å¾—
                    soup = self.fetch_detail_page(url)
                    
                    # ç‰©ä»¶æƒ…å ±ã‚’æŠ½å‡º
                    d = self.extract_detail(soup, url)
                    
                    # ç”»åƒã‚’å–å¾—
                    if d['ç‰©ä»¶ç•ªå·']:
                        img_urls = self.extract_images(soup, d['ç‰©ä»¶ç•ªå·'])
                        d['ç”»åƒURL'] = ', '.join(img_urls) if img_urls else ''
                        d['ç”»åƒæšæ•°'] = len(img_urls)
                        
                        # å–å¾—æ¸ˆã¿ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        self.scraped_ids.add(d['ç‰©ä»¶ç•ªå·'])
                        new_count += 1
                    else:
                        d['ç”»åƒURL'] = ''
                        d['ç”»åƒæšæ•°'] = 0
                    
                    rows.append(d)
                    self.log(f"  âœ“ ç‰©ä»¶ç•ªå· {d['ç‰©ä»¶ç•ªå·']} - ç”»åƒ{d['ç”»åƒæšæ•°']}æšå–å¾—")
                    
                    # ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’ç©ºã‘ã‚‹
                    if idx < len(detail_urls):
                        time.sleep(1)
                    
                except Exception as ex:
                    self.log(f"  âŒ ã‚¨ãƒ©ãƒ¼: {ex}")
                    continue
            
            self.update_progress(len(detail_urls), len(detail_urls))
            
            # å±¥æ­´ã‚’ä¿å­˜
            self.save_history()
            self.scraped_count_label.config(text=f"ï¼ˆå–å¾—æ¸ˆã¿: {len(self.scraped_ids)}ä»¶ï¼‰")
            
            if not rows:
                self.log("\nâš ï¸ æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                self.log(f"  ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶")
                self.finish_scraping(True)
                return
            
            # CSV/JSONå‡ºåŠ›
            self.log("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ä¸­...")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
            if os.path.exists(CSV_FILE):
                existing_df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')
                new_df = pd.DataFrame(rows)
                df = pd.concat([existing_df, new_df], ignore_index=True)
                # ç‰©ä»¶ç•ªå·ã§é‡è¤‡å‰Šé™¤ï¼ˆæœ€æ–°ã‚’ä¿æŒï¼‰
                df = df.drop_duplicates(subset=['ç‰©ä»¶ç•ªå·'], keep='last')
            else:
                df = pd.DataFrame(rows)
            
            df.to_csv(CSV_FILE, index=False, encoding='utf-8-sig')
            self.log(f"  âœ“ CSVå‡ºåŠ›å®Œäº†: {CSV_FILE}")
            
            # JSONå‡ºåŠ›
            all_data = df.to_dict('records')
            with open(JSON_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            self.log(f"  âœ“ JSONå‡ºåŠ›å®Œäº†: {JSON_FILE}")
            
            # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã®ç¢ºèª
            if os.path.exists(IMG_FOLDER):
                img_files = os.listdir(IMG_FOLDER)
                self.log(f"  âœ“ ç”»åƒä¿å­˜å®Œäº†: {IMG_FOLDER}/ ({len(img_files)}ãƒ•ã‚¡ã‚¤ãƒ«)")
            
            self.log("\n" + "=" * 60)
            self.log(f"ğŸ‰ å®Œäº†:")
            self.log(f"  æ–°è¦å–å¾—: {new_count}ä»¶")
            self.log(f"  ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶")
            self.log(f"  åˆè¨ˆ: {len(df)}ä»¶ï¼ˆCSV/JSONï¼‰")
            self.log("=" * 60)
            
            self.finish_scraping(True)
            
        except Exception as ex:
            self.log(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {ex}")
            self.finish_scraping(False)
    
    def finish_scraping(self, success):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ‚äº†å‡¦ç†"""
        self.is_running = False
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
        self.max_items_entry.config(state=tk.NORMAL)
        self.skip_checkbox.config(state=tk.NORMAL)
        self.clear_history_button.config(state=tk.NORMAL)
        
        if success:
            self.status_label.config(text="å®Œäº†")
            messagebox.showinfo("å®Œäº†", "ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n\nå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:\n- export.csv\n- export.json\n- images/")
            
            # çµæœãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã
            if messagebox.askyesno("ç¢ºèª", "çµæœãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãã¾ã™ã‹ï¼Ÿ"):
                self.open_output_folder()
        else:
            self.status_label.config(text="ã‚¨ãƒ©ãƒ¼")
    
    def open_output_folder(self):
        """å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã"""
        try:
            if sys.platform == 'win32':
                os.startfile(os.getcwd())
            elif sys.platform == 'darwin':
                os.system(f'open "{os.getcwd()}"')
            else:
                os.system(f'xdg-open "{os.getcwd()}"')
        except Exception as ex:
            self.log(f"ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {ex}")
    
    # ä»¥ä¸‹ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰
    
    def fetch_list_page(self):
        res = requests.get(LIST_URL, headers=HEADERS)
        return BeautifulSoup(res.text, 'html.parser')
    
    def get_detail_urls(self, soup, max_items):
        urls = []
        scripts = soup.find_all('script', {'type':'application/ld+json'})
        for script in scripts:
            if 'ItemList' in script.text:
                try:
                    data = json.loads(script.text)
                    for item in data.get('itemListElement', []):
                        url = item.get('item')
                        urls.append(url)
                        if len(urls) >= max_items:
                            break
                except Exception:
                    pass
        return urls
    
    def fetch_detail_page(self, url):
        detail_headers = HEADERS.copy()
        detail_headers["Referer"] = LIST_URL
        res = requests.get(url, headers=detail_headers)
        return BeautifulSoup(res.text, 'html.parser')
    
    def extract_images(self, soup, estate_id):
        img_urls = []
        
        # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚¿ã‚°ã‹ã‚‰å–å¾—
        img_tags = soup.select('a.mainContents_gallery-colorbox')
        
        # preloadã‚¿ã‚°ã‹ã‚‰å–å¾—
        preload_tags = soup.select('link[rel="preload"][as="image"]')
        
        if img_tags:
            for tag in img_tags:
                href = tag.get('href')
                if href:
                    img_urls.append(href)
        elif estate_id and preload_tags:
            for tag in preload_tags:
                href = tag.get('href', '')
                if estate_id in href and 'exterior' in href.lower():
                    img_urls.append(href)
        
        for idx, img_url in enumerate(img_urls):
            img_url = urllib.parse.urljoin(BASE_URL, img_url)
            
            try:
                res = requests.get(img_url, headers=HEADERS, timeout=10)
                
                if res.status_code == 200:
                    img_ext = img_url.split('.')[-1].split('?')[0]
                    if img_ext not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                        img_ext = 'jpg'
                    
                    img_name = f"{estate_id}_{idx+1}.{img_ext}"
                    img_dir = IMG_FOLDER
                    os.makedirs(img_dir, exist_ok=True)
                    
                    img_path = os.path.join(img_dir, img_name)
                    with open(img_path, 'wb') as f:
                        f.write(res.content)
            except Exception:
                pass
            
            time.sleep(0.5)
        
        return [urllib.parse.urljoin(BASE_URL, url) for url in img_urls]
    
    def extract_detail(self, soup, url):
        estate_elem = soup.find('td', class_='estateID')
        estate_id = estate_elem.text.strip() if estate_elem else ''
        
        table = soup.find('table')
        data = {}
        
        if table:
            all_cells = table.find_all(['th', 'td'])
            
            i = 0
            while i < len(all_cells):
                cell = all_cells[i]
                
                if cell.name == 'th':
                    key = cell.text.strip()
                    j = i + 1
                    while j < len(all_cells):
                        next_cell = all_cells[j]
                        if next_cell.name == 'td':
                            value = next_cell.text.strip()
                            data[key] = value
                            i = j
                            break
                        elif next_cell.name == 'th':
                            break
                        j += 1
                
                i += 1
        
        summary = {
            'ç‰©ä»¶ç•ªå·': estate_id,
            'ä¾¡æ ¼': data.get('ä¾¡æ ¼', ''),
            'é–“å–ã‚Š': data.get('é–“å–ã‚Š', ''),
            'ç‰©ä»¶ç¨®åˆ¥': data.get('ç‰©ä»¶ç¨®åˆ¥', ''),
            'æ‰€åœ¨åœ°': data.get('æ‰€åœ¨åœ°', ''),
            'ã‚¢ã‚¯ã‚»ã‚¹': data.get('ã‚¢ã‚¯ã‚»ã‚¹', ''),
            'å»ºç‰©é¢ç©': data.get('å»ºç‰©é¢ç©', ''),
            'é§è»Šå ´': data.get('é§è»Šå ´', ''),
            'ç¯‰å¹´æœˆ': data.get('ç¯‰å¹´æœˆ', ''),
            'å»ºç‰©æ§‹é€ ': data.get('å»ºç‰©æ§‹é€ ', ''),
            'å·¥æ³•': data.get('å·¥æ³•', ''),
            'ä¸»è¦æ¡å…‰': data.get('ä¸»è¦æ¡å…‰', ''),
            'ãƒãƒ«ã‚³ãƒ‹ãƒ¼': data.get('ãƒãƒ«ã‚³ãƒ‹ãƒ¼', ''),
            'ä¿è¨¼ãƒ»è©•ä¾¡': data.get('ä¿è¨¼ãƒ»è©•ä¾¡', ''),
            'ãƒªãƒ•ã‚©ãƒ¼ãƒ ': data.get('ãƒªãƒ•ã‚©ãƒ¼ãƒ ', ''),
            'åœŸåœ°é¢ç©': data.get('åœŸåœ°é¢ç©', ''),
            'æ¥é“': data.get('æ¥é“', ''),
            'ã‚»ãƒƒãƒˆãƒãƒƒã‚¯': data.get('ã‚»ãƒƒãƒˆãƒãƒƒã‚¯', ''),
            'ç§é“': data.get('ç§é“', ''),
            'åœ°ç›®': data.get('åœ°ç›®', ''),
            'åœ°å‹¢': data.get('åœ°å‹¢', ''),
            'æ¨©åˆ©': data.get('æ¨©åˆ©', ''),
            'éƒ½å¸‚è¨ˆç”»': data.get('éƒ½å¸‚è¨ˆç”»', ''),
            'ç”¨é€”åœ°åŸŸ': data.get('ç”¨é€”åœ°åŸŸ', ''),
            'å»ºãºã„/å®¹ç©ç‡': data.get('å»ºãºã„/å®¹ç©ç‡', ''),
            'åœŸåœ°å›½åœŸæ³•': data.get('åœŸåœ°å›½åœŸæ³•', ''),
            'è¨±å¯ç•ªå·': data.get('è¨±å¯ç•ªå·', ''),
            'å»ºç¯‰åŸºæº–æ³•': data.get('å»ºç¯‰åŸºæº–æ³•', ''),
            'æ³•ä»¤åˆ¶é™': data.get('æ³•ä»¤åˆ¶é™', ''),
            'å°å­¦åŒº': data.get('å°å­¦åŒº', ''),
            'ä¸­å­¦åŒº': data.get('ä¸­å­¦åŒº', ''),
            'ç¾æ³': data.get('ç¾æ³', ''),
            'å¼•æ¸¡': data.get('å¼•æ¸¡', ''),
            'ãã®ä»–è²»ç”¨': data.get('ãã®ä»–è²»ç”¨', ''),
            'å‚™è€ƒ': data.get('å‚™è€ƒ', ''),
            'å–å¼•æ…‹æ§˜': data.get('å–å¼•æ…‹æ§˜', ''),
            'è©³ç´°ãƒšãƒ¼ã‚¸': url
        }
        return summary


def main():
    root = tk.Tk()
    app = ScraperGUI(root)
    root.mainloop()


if __name__ == '__main__':
    main()

